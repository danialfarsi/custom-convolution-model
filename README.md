# custom-convolution-model
In this example, I trained and tested it using the MNIST dataset and building a custom convolution model.

MNIST Dataset

![mnist-3 0 1](https://user-images.githubusercontent.com/43178887/134819812-f6fd5a37-1d84-4a4d-84a8-60a25d72e1e5.png)


The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset. It is a dataset of 60,000 small square 28Ã—28 pixel grayscale images of handwritten single digits between 0 

Then I taught it using this custom model and did not use the Pre-trained models. After that, I examined two optimizations whose output was separated.


ADAM Optimizer :

Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.


<img width="249" alt="Screen Shot 1400-07-04 at 21 39 22" src="https://user-images.githubusercontent.com/43178887/134819910-4c27dcc6-2560-4bb4-92ed-d230c81509ef.png">


SGD Optimizer :

Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. 


<img width="262" alt="Screen Shot 1400-07-04 at 21 52 05" src="https://user-images.githubusercontent.com/43178887/134819944-e0484bd9-347b-441b-a884-4159b18b2cd9.png">
